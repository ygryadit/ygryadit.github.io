- title: "SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time"
  authors: "Zhening Huang, Hyeonho Jeong, Xuelin Chen, <b>Yulia Gryaditskaya</b>, Tuanfeng Y. Wang, Joan Lasenby, Chun-Hao Huang"
  venue: "Preprint"
  year: 2026  
  paper_link: "https://arxiv.org/abs/2512.25075"
  preview_img_link: "./Papers/2026/SpaceTimePilot/preview.png"
  img_width: '130px'
  project_link: "https://zheninghuang.github.io/Space-Time-Pilot/"
  bibtex_link: ""
  abstract: "We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. 
    Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, 
    re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animation 
    time-embedding mechanism in the diffusion process, allowing explicit control of the output video's motion sequence with respect to that of 
    the source video. As no datasets provide paired videos of the same dynamic scene with continuous temporal variations, we propose a simple 
    yet effective temporal-warping training scheme that repurposes existing multi-view datasets to mimic temporal differences. This strategy 
    effectively supervises the model to learn temporal control and achieve robust space-time disentanglement. To further enhance the precision 
    of dual control, we introduce two additional components: an improved camera-conditioning mechanism that allows altering the camera from the 
    first frame, and CamxTime, the first synthetic space-and-time full-coverage rendering dataset that provides fully free space-time video 
    trajectories within a scene. Joint training on the temporal-warping scheme and the CamxTime dataset yields more precise temporal control. 
    We evaluate SpaceTimePilot on both real-world and synthetic data, demonstrating clear space-time disentanglement and strong results compared to prior work."
  publ_id: "2026_2"
  type: "p"

- title: "SketchingReality: From Freehand Scene Sketches to Photorealistic Images"
  authors: "Ahmed Bourouis, Mikhail Bessmeltsev, <b>Yulia Gryaditskaya</b>"
  venue: "ICLR"
  year: 2026  
  paper_link: ""
  preview_img_link: "./Papers/2026/SketchingReality/preview.png"
  img_width: '130px'
  project_link: "https://ahmedbourouis.github.io/SketchingReality_ICLR26/"
  bibtex_link: ""
  abstract: "Recent years have witnessed remarkable progress in generative AI, with natural language emerging as the most common conditioning input. 
      As underlying models grow more powerful, researchers are exploring increasingly diverse conditioning signals—such as depth maps, edge maps, 
      camera parameters, and reference images—to give users finer control over generation. Among different modalities, sketches are a natural and long-standing 
      form of human communication, enabling rapid expression of visual concepts. Previous literature has largely focused on edge maps, often misnamed 
      ''sketches''; however, algorithms that effectively handle true freehand sketches-with their inherent abstraction and distortions-remain underexplored.
      We pursue the challenging goal of balancing photorealism with sketch adherence when generating images from freehand input. A key obstacle is the absence 
      of ground-truth, pixel-aligned images: by their nature, freehand sketches do not have a single correct alignment. 
      To address this, we propose a modulation-based approach that prioritizes semantic interpretation of the sketch over strict adherence to individual edge positions. 
      We further introduce a novel loss that enables training on freehand sketches without requiring ground-truth pixel-aligned images. We show that our method 
      outperforms existing approaches in both semantic alignment with freehand sketch inputs and in the realism and overall quality of the generated images."
  publ_id: "2026_1"
  type: "c"

- title: "GroundUp: Rapid Sketch-Based 3D City Massing"
  authors: "Gizem Unlu, Mohamed Sayed, <b>Yulia Gryaditskaya</b>, Gabriel Brostow"
  venue: "ECCV"
  year: 2024  
  paper_link: "http://visual.cs.ucl.ac.uk/pubs/groundup/GroundUp_paper.pdf"
  preview_img_link: "./Papers/24_ECCV/GroundUp/preview.png"
  img_width: '130px'
  project_link: "http://visual.cs.ucl.ac.uk/pubs/groundup/index.html"
  bibtex_link: ""
  abstract: "We propose GroundUp, the first sketch-based ideation tool
                for 3D city massing of urban areas. We focus on early-stage urban design,
                where sketching is a common tool and the design starts from balancing
                building volumes (masses) and open spaces. With Human-Centered AI in
                mind, we aim to help architects quickly revise their ideas by easily switching between 2D sketches and 3D models, allowing for smoother iteration
                and sharing of ideas. Inspired by feedback from architects and existing
                workflows, our system takes as a first input a user sketch of multiple
                buildings in a top-down view. The user then draws a perspective sketch
                of the envisioned site. Our method is designed to exploit the complementarity of information in the two sketches and allows users to quickly
                preview and adjust the inferred 3D shapes. Our model has two main components. First, we propose a novel sketch-to-depth prediction network for
                perspective sketches that exploits top-down sketch shapes. Second, we use
                depth cues derived from the perspective sketch as a condition to our diffusion model, which ultimately completes the geometry in a top-down
                view. Thus, our final 3D geometry is represented as a heightfield, allowing users to construct the city “from the ground up”."
  publ_id: "2024_4"
  type: "c"
  
- title: "Open Vocabulary Scene Sketch Semantic Understanding"
  authors: "Ahmed Bourouis, Judith Ellen Fan, <b>Yulia Gryaditskaya</b>"
  venue: "CVPR"
  year: 2024  
  paper_link: "https://ahmedbourouis.github.io/Scene_Sketch_Segmentation/"
  preview_img_link: "./Papers/24_CVPR/OVSSS/preview-01.png"
  img_width: '130px'
  project_link: "https://ahmedbourouis.github.io/Scene_Sketch_Segmentation/"
  bibtex_link: ""
  abstract: "We study the underexplored but fundamental vision problem of machine understanding of abstract freehand scene sketches. We introduce a sketch encoder that results in semantically- aware feature space, which we evaluate by testing its performance on a semantic sketch seg- mentation task. To train our model we rely only on the availability of bitmap sketches with their brief captions and do not require any pixel-level annotations. To obtain generalization to a large set of sketches and categories, we build on a vision transformer encoder pretrained with the CLIP model. We freeze the text encoder and perform visual-prompt tuning of the visual encoder branch while introducing a set of critical modifications. Firstly, we augment the classical key-query (k-q) self-attention blocks with value-value (v-v) self-attention blocks. Central to our model is a two-level hierarchical network design that enables efficient semantic disentanglement: The first level ensures holistic scene sketch encoding, and the second level focuses on individual categories. We, then, in the second level of the hierarchy, introduce a cross-attention between textual and visual branches. Our method outperforms zero-shot CLIP pixel accuracy of segmentation results by 37 points, reaching an accuracy of 85.5% on the FS-COCO sketch dataset. Finally, we conduct a user study that allows us to identify further improvements needed over our method to reconcile machine and human understanding of scene sketches."
  publ_id: "2024_3"
  type: "c"
  
- title: "Modeling Complex Vector Drawings with Stroke Clouds"
  authors: "Alexander Ashcroft, Ayan Das, <b>Yulia Gryaditskaya</b>, Zhiyu Qu, Yi-Zhe Song"
  venue: "ICLR"
  year: 2024  
  paper_link: "https://openreview.net/pdf?id=O2jyuo89CK"
  preview_img_link: "./Papers/24_ICLR/preview.png"
  img_width: '130px'
  project_link: "https://github.com/Co-do/Stroke-Cloud"
  bibtex_link: ""
  abstract: "   Vector representations offer scalability, editability, and storage efficiency, making them indispensable for a wide range of digital applications.
                Yet, generative models for vector drawings remain under-explored, in particular for modeling complex vector drawings. 
                This is in part due to the primarily sequential and auto-regressive nature of existing approaches failing to scale beyond simple drawings. 
                In this paper, we introduce a generative model for complex vector drawings, representing them as ``stroke clouds'' -- sets of arbitrary cardinality comprised of n-dimensional Bézier curves. 
                Stroke dimensionality is a design choice that allows the model to adapt to different levels of sketch complexity. 
                We learn to encode this set of strokes into compact latent codes by a probabilistic reconstruction procedure based on De-Finetti's Theorem of Exchangeability. 
                A generative model is then defined over the latent vectors of the encoded stroke clouds. 
                Thus, the resulting ``Latent stroke cloud generator (LSG)'' captures the distribution of complex vector drawings in an implicit set space.
                We demonstrate the efficacy of our model in the generation of complex Anime line-art drawings."
  publ_id: "2024_2"
  type: "c"
  
  
- title: "3D Reconstruction from Sketch with Hidden Lines by Two-Branch Diffusion Model"
  authors: "Yuta Fukushima, Anran Qi, I-Chao Shen, <b>Yulia Gryaditskaya</b>, Takeo Igarashi"
  venue: "Eurographics 2024 Short Paper"
  year: 2024  
  paper_link: "https://diglib.eg.org/items/f669041e-4a18-4406-88d4-e9c20cad7cc9"
  preview_img_link: "./Papers/24_EG/preview.png"
  img_width: '130px'
  project_link: ""
  bibtex_link: ""
  abstract: "   We present a method for sketch-based modelling of 3D man-made shapes that exploits not only the commonly considered visible surface lines but also the hidden lines typical for technical drawings.
                Hidden lines are used by artists and designers to communicate holistic shape structure. 
                Given a single viewpoint sketch, leveraging such lines allows us to resolve the ambiguity of the shape's surfaces hidden from the observer.
                We assume that the separation into visible and hidden lines is given, and focus solely on how to leverage this information. 
                Our strategy is to mingle two distinct diffusion networks: one generates denoized occupancy grid estimates from a visible line image, whilst the other generates occupancy grid estimates based on contextualized hidden lines unveiling the occluded shape structure. 
                We iteratively merge noisy estimates from both models in a reverse diffusion process. 
                Importantly, we demonstrate the importance of what we call a contextualized hidden lines image over just a hidden lines image. 
                Our contextualized hidden lines image contains hidden lines and silhouette lines. Such contextualization allows us to achieve superior performance to a range of alternative configurations and reconstruct hidden holes and hidden surfaces."
  publ_id: "2024_1"
  type: "c"
  
  
- title: "3D VR Sketch Guided 3D Shape Prototyping and Exploration"
  authors: "Ling Luo, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song, <b>Yulia Gryaditskaya</b>"
  venue: "ICCV"
  year: 2023  
  paper_link: "https://arxiv.org/abs/2306.10830"
  preview_img_link: "./Papers/23_ICCV/3DPrototype/preview.png"
  img_width: '130px'
  project_link: "https://github.com/Rowl1ng/3Dsketch2shape"
  bibtex_link: ""
  abstract: "3D shape modeling is labor-intensive, time-consuming, and requires years of expertise. To facilitate 3D shape modeling, we propose a 3D shape generation network that takes a 3D VR sketch as a condition. We assume that sketches are created by novices without art training and aim to reconstruct geometrically realistic 3D shapes of a given category. To handle potential sketch ambiguity, our method creates multiple 3D shapes that align with the original sketch’s structure. We carefully design our method, training the model step-by-step and leveraging multi-modal 3D shape representation to support training with limited training data. To guarantee the realism of generated 3D shapes we leverage the normalizing flow that models the distribution of the latent space of 3D shapes. To encourage the fidelity of the generated 3D shapes to an input sketch, we propose a dedicated loss that we deploy at different stages of the training process."
  publ_id: "2023_3"
  type: "c"
  
- title: "Fine-Tuned but Zero-Shot 3D Shape Sketch View Similarity and Retrieval"
  authors: "Gianluca Berardi, <b>Yulia Gryaditskaya</b>"
  venue: "ICCV SHARP Workshop (ICCVW)"
  year: 2023  
  paper_link: "https://arxiv.org/abs/2306.08541"
  preview_img_link: "./Papers/23_ICCV/Zero_shot_but_fine_tuned/preview.PNG"
  img_width: '130px'
  project_link: ""
  bibtex_link: ""
  abstract: "Recently, encoders like ViT (vision transformer) and ResNet have been trained on vast datasets and utilized as perceptual metrics for comparing sketches and images, as well as multi-domain encoders in a zero-shot setting. However, there has been limited effort to quantify the granularity of these encoders. Our work addresses this gap by focusing on multi-modal 2D projections of individual 3D instances. This task holds crucial implications for retrieval and sketch-based modeling. We show that in a zero-shot setting (without retraining on a specific shape category or sketch style), the more abstract the sketch, the higher the likelihood of incorrect image matches. Even within the same sketch domain, sketches of the same object drawn in different styles, for example by distinct individuals, might not be accurately matched.
One of the key findings of our research is that meticulous fine-tuning on one class of 3D shapes leads to improved performance on other shape classes (fine-tuned but zero-shot), reaching or surpassing the accuracy of supervised methods. We compare and discuss several fine-tuning strategies. Additionally, we delve deeply into how the scale of an object in a sketch influences the similarity of features at different network layers, helping us identify which network layers provide the most accurate matching.
Significantly, we discover that ViT and ResNet perform best when dealing with similar object scales. We believe that our work will have a significant impact on research in the sketch domain, providing insights and guidance on how to adopt large pretrained models as perceptual losses."
  publ_id: "2023_2"
  type: "c"
  
- title: "SketchXAI: A First Look at Explainability for Human Sketches"
  authors: "Zhiyu Qu, <b>Yulia Gryaditskaya</b>, Ke Li, Kaiyue Pang, Tao Xiang, Yi-Zhe Song"
  venue: "CVPR"
  year: 2023  
  paper_link: "https://arxiv.org/abs/2304.11744"
  preview_img_link: "./Papers/23_CVPR/SketchXAI/preview.png"
  img_width: '130px'
  project_link: "https://sketchxai.github.io/"
  bibtex_link: "/Papers/bibtex_entries/23_CVPR_SketchXAI.txt"
  abstract: "This paper, for the very first time, introduces human sketches to the landscape of XAI (Explainable Artificial Intelligence). We argue that sketch as a human-centered data form, represents a natural interface to study explainability. We focus on cultivating sketch-specific explainability designs. This starts by identifying strokes as a unique building block that offers a degree of flexibility in object construction and manipulation impossible in photos. Following this, we design a simple explainability-friendly sketch encoder that accommodates the intrinsic properties of strokes: shape, location, and order. We then define the first ever XAI task for sketch, that of stroke location inversion (SLI). Just as we have heat maps for photos and correlation matrices for text, SLI offers an explainability angle to sketch by asking a network how well it can recover stroke locations of an unseen sketch. We provide qualitative results for readers to interpret as snapshots of the SLI process in the paper and as GIFs on the project page. A minor but exciting note is that thanks to its sketch-specific design, our sketch encoder also yields the best sketch recognition accuracy to date while having the smallest number of parameters."
  publ_id: "2023_1"
  type: "c"
  
- title: "Garment Ideation: Iterative View-Aware Sketch-Based Garment Modeling"
  authors: "Pinaki Nath Chowdhury, Tuanfeng Wang, Duygu Ceylan, Yi-Zhe Song, <b>Yulia Gryaditskaya</b>"
  venue: "Proceedings of International Conference on 3D Vision (3DV) (oral)"
  year: 2022  
  paper_link: "http://www.pinakinathc.me/assets/papers/3DV_2022.pdf"
  preview_img_link: "./Papers/22_3DV/Garment/preview.png"
  img_width: '130px'
  project_link: "https://github.com/pinakinathc/multiviewsketch-garment"
  bibtex_link: "/Papers/bibtex_entries/22_3DV_Garment.txt"
  abstract: "Designing real and virtual garments is becoming extremely demanding with rapidly changing fashion trends and increasing need for synthesizing realistically dressed digital humans for various applications. However, traditionally designing real and virtual garments has been time-consuming. Sketch based modeling aims to bring the ease and immediacy of drawing to the 3D world thereby motivating faster iterations. We propose a novel sketch-based garment modeling framework that is specifically targeted to synchronize with the iterative process of garment ideation, e.g., adding or removing details from different views in each iteration. At the core of our learning based approach is a view-aware feature aggregation module that fuses the features from the latest sketch with the thus far aggregated features to effective refine the generated 3D shape. We evaluate our approach on a wide variety of garment types and iterative refinement scenarios. We also provide comparisons to alternative feature aggregation methods and demonstrate favorable results."
  publ_id: "2022_6"
  type: "c"

- title: "Structure-Aware 3D VR Sketch to 3D Shape Retrieval"
  authors: "Ling Luo, <b>Yulia Gryaditskaya</b>, Tao Xiang, Yi-Zhe Song"
  venue: "Proceedings of International Conference on 3D Vision (3DV)"
  year: 2022  
  paper_link: "https://arxiv.org/pdf/2209.09043v1.pdf"
  preview_img_link: "./Papers/22_3DV/VRSketch/preview.png"
  img_width: '130px'
  project_link: "https://github.com/Rowl1ng/Structure-Aware-VR-Sketch-Shape-Retrieval"
  bibtex_link: ""
  abstract: "We study the practical task of fine-grained 3D-VR-sketch-based 3D shape retrieval. This task is of particular interest as 2D sketches were shown to be effective queries for 2D images. However, due to the domain gap, it remains hard to achieve strong performance in 3D shape retrieval from 2D sketches. Recent work demonstrated the advantage of 3D VR sketching on this task. 
In our work, we focus on the challenge caused by inherent inaccuracies in 3D VR sketches.
We observe that retrieval results obtained with a triplet loss with a fixed margin value, commonly used for retrieval tasks, contain many irrelevant shapes and often just one or few with a similar structure to the query.
To mitigate this problem, we for the first time draw a connection between adaptive margin values and shape similarities.
In particular, we propose to use a triplet loss with an adaptive margin value driven by a `fitting gap', which is the similarity of two shapes under structure-preserving deformations. 
We also conduct a user study which confirms that this fitting gap is indeed a suitable criterion to evaluate the structural similarity of shapes. 
Furthermore, we introduce a dataset of 202 VR sketches for 202 3D shapes drawn from memory rather than from observation. "
  publ_id: "2022_5"
  type: "c"
  
  

- title: "FS-COCO: Towards Understanding of Freehand Sketches of Common Objects in Context"
  authors: "Pinaki Nath Chowdhury, Aneeshan Sain, Ayan Kumar Bhunia, Tao Xiang, <b>Yulia Gryaditskaya</b>, Yi-Zhe Song"
  venue: "ECCV"
  year: 2022  
  paper_link: "https://arxiv.org/pdf/2203.02113.pdf"
  preview_img_link: "./Papers/22_ECCV/thumb.png"
  img_width: '130px'
  project_link: "https://fscoco.github.io/"
  bibtex_link: ""
  abstract: "We advance sketch research to scenes with the first dataset of freehand scene sketches, FS-COCO. With practical applications in mind, we collect sketches that convey scene content well but can be sketched within a few minutes by a person with any sketching skills. Our dataset comprises 10, 000 freehand scene vector sketches with per point spacetime information by 100 non-expert individuals, offering both objectand scene-level abstraction. Each sketch is augmented with its text description. Using our dataset, we study for the first time the problem of fine-grained image retrieval from freehand scene sketches and sketch captions. We draw insights on: (i) Scene salience encoded in sketches using the strokes temporal order; (ii) Performance comparison of image retrieval from a scene sketch and an image caption; (iii) Complementarity of information in sketches and image captions, as well as the potential benefit of combining the two modalities. In addition, we extend a popular vector sketch LSTM-based encoder to handle sketches with larger complexity than was supported by previous work. Namely, we propose a hierarchical sketch decoder, which we leverage at a sketch-specific “pretext” task. Our dataset enables for the first time research on freehand scene sketch understanding and its practical applications. We release the dataset under CC BY-NC 4.0 license."
  publ_id: "2022_4"
  type: "c"
  
- title: "Symmetry-driven 3D Reconstruction From Concept Sketches"
  authors: "Felix Hahnlein, <b>Yulia Gryaditskaya</b>, Alla Sheffer, Adrien Bousseau"
  venue: "SIGGRAPH North America"
  year: 2022  
  paper_link: "https://repo-sam.inria.fr/d3/SymmetrySketch/symmetry_sketch.pdf"
  preview_img_link: "./Papers/22_siggraph_symmetry/papers_442s3.jpg"
  img_width: '130px'
  project_link: "https://ns.inria.fr/d3/SymmetrySketch/"
  bibtex_link: ""
  abstract: "Concept sketches, ubiquitously used in industrial design, are inherently imprecise yet highly effective at communicating 3D shape to human observers. We present a new symmetry-driven algorithm for recovering designer-intended 3D geometry from concept sketches. We observe that most concept sketches of human-made shapes are structured around locally symmetric building blocks, defined by triplets of orthogonal symmetry planes. We identify potential building blocks using a combination of 2D symmetries and drawing order. We reconstruct each such building block by leveraging a combination of perceptual cues and observations about designer drawing choices. We cast this reconstruction as an integer programming problem where we seek to identify, among the large set of candidate symmetry correspondences formed by approximate pen strokes, the subset that results in the most symmetric and well-connected shape.We demonstrate the robustness of our approach by reconstructing 82 sketches, which exhibit significant over-sketching, inaccurate perspective, partial symmetry, and other imperfections. In a comparative study, participants judged our results as superior to the state-of-the-art by a ratio of 2:1."
  publ_id: "2022_3"
  type: "c"
  

- title: "One Sketch for All: One-Shot Personalized Sketch Segmentation"
  authors: "Anran Qi, <b>Yulia Gryaditskaya</b>, Tao Xiang, Yi-Zhe Song"
  venue: "IEEE Transactions on Image Processing"
  year: 2022  
  paper_link: "https://arxiv.org/abs/2112.10838"
  preview_img_link: "./Papers/22_Sketch_Segmentation/preview.png"
  img_width: '130px'
  bibtex_link: "/Papers/22_Sketch_Segmentation/qi2021OneSketch4All.txt"
  abstract: "We present the first one-shot personalized sketch segmentation method. We aim to segment all sketches belonging to the same category provisioned with a single sketch with a given part annotation while (i) preserving the parts semantics embedded in the exemplar, and (ii) being robust to input style and abstraction. We refer to this scenario as personalized. With that, we importantly enable a much-desired personalization capability for downstream fine-grained sketch analysis tasks. To train a robust segmentation module, we deform the exemplar sketch to each of the available sketches of the same category. Our method generalizes to sketches not observed during training. Our central contribution is a sketch-specific hierarchical deformation network. Given a multi-level sketch-strokes encoding obtained via a graph convolutional network, our method estimates rigid-body transformation from the reference to the exemplar, on the upper level. Finer deformation from the exemplar to the globally warped reference sketch is further obtained through stroke-wise deformations, on the lower level. Both levels of deformation are guided by mean squared distances between the keypoints learned without supervision, ensuring that the stroke semantics are preserved. We evaluate our method against the state-of-the-art segmentation and perceptual grouping baselines re-purposed for the one-shot setting and against two few-shot 3D shape segmentation methods. We show that our method outperforms all the alternatives by more than 10% on average. Ablation studies further demonstrate that our method is robust to personalization: changes in input part semantics and style differences."
  publ_id: "2022_2"
  type: "j"  

- title: "A Study of Deep Single Sketch-Based Modeling: View/Style Invariance, Sparsity and Latent Space Disentanglement"
  authors: "Yue Zhong, <b>Yulia Gryaditskaya</b>, Honggang Zhang, Yi-Zhe Song"
  venue: "Computers &amp; Graphics"
  year: 2022  
  paper_link: "https://www.sciencedirect.com/science/article/pii/S0097849322001078"
  preview_img_link: "./Papers/22_SBM_journal/preview.png"
  project_link: "https://github.com/Yueeey/deepsketchSDFs"
  img_width: '130px'
  bibtex_link: "/Papers/22_SBM_journal/22_SBM_journal.txt"
  abstract: "Deep image-based modeling has received a lot of attention in recent years. Sketch-based modeling in particular has gained popularity given the ubiquitous nature of touchscreen devices. In this paper, we (i) study and compare diverse single-image reconstruction methods on sketch input, comparing the different 3D shape representations: multi-view, voxel- and point-cloud-based, mesh-based and implicit ones; and (ii) analyze the main challenges and requirements of sketch-based modeling systems. We introduce the regression loss and provide two variants of its formulation for the two most promising 3D shape representations: point clouds and signed distance functions. We show that this loss can increase general reconstruction accuracy, and the view- and style-robustness of the reconstruction methods. Moreover, we demonstrate that this loss can benefit the disentanglement of latent space to view-invariant and view-specific information, resulting in further improved performance. To address the figure-ground ambiguity typical for sparse human sketches, we propose a two-branch architecture that exploits sparse user labeling. We hope that our work will inform future research on sketch-based modeling. We will release our datasets and their splits to establish the first benchmark in sketch- based modeling."
  publ_id: "2022_1"
  doi: "https://doi.org/10.1016/j.cag.2022.06.005"
  type: "j"
  
- title: "Fine-Grained VR Sketching: Dataset and Insights"
  authors: "Ling Luo, <b>Yulia Gryaditskaya</b>, Yongxin Yang, Tao Xiang, Yi-Zhe Song"
  venue: "Proceedings of International Conference on 3D Vision (3DV)"
  year: 2021  
  paper_link: "https://arxiv.org/pdf/2209.10008.pdf"
  preview_img_link: "./Papers/3DV_21/preview.PNG"
  img_width: '160px'  
  project_link: "https://tinyurl.com/VRSketch3DV21"
  bibtex_link: "/Papers/3DV_21/luo2021FineGrainedVR.txt"  
  abstract: "We present the first fine-grained dataset of 1,497 3D VR sketch and 3D shape pairs of a chair category with large shapes diversity. Our dataset supports the recent trend in the sketch community on fine-grained data analysis, and extends it to an actively developing 3D domain. We argue for the most convenient sketching scenario where the sketch consists of sparse lines and does not require any sketching skills, prior training or time-consuming accurate drawing. We then, for the first time, study the scenario of fine-grained 3D VR sketch to 3D shape retrieval, as a novel VR sketching application and a proving ground to drive out generic insights to inform future research. By experimenting with carefully selected combinations of design factors on this new problem, we draw important conclusions to help follow-on work. We hope our dataset will enable other novel applications, especially those that require a fine-grained angle such as fine-grained 3D shape reconstruction."
  publ_id: "2021_2"
  type: "c"
  
- title: "Towards Fine-Grained Sketch-Based 3D Shape Retrieval"
  title_link: ""
  authors: "Anran Qi, <b>Yulia Gryaditskaya</b>, Jifei Song, Yongxin Yang, Yonggang Qi, Timothy M. Hospedales, Tao Xiang, Yi-Zhe Song"
  venue: "IEEE Transactions on Image Processing"
  year: 2021  
  paper_link: "https://ieeexplore.ieee.org/document/9573376?source=authoralert"
  preview_img_link: "./Papers/TIP_Retrieval/preview.png"
  img_width: '130px'
  bibtex_link: "/Papers/TIP_Retrieval/qi2021Retrieval.txt"
  abstract: "In this paper we study, for the first time, the problem1of fine-grained sketch-based 3D shape retrieval. We advocate the use of sketches as a fine-grained input modality to retrieve 3D shapes at instance-level e.g., given a sketch of a chair, we set out to retrieve a specific chair from a gallery of all chairs. Fine-grained sketch-based 3D shape retrieval (FG-SBSR) has not been possible till now due to a lack of datasets that exhibit one-to-one sketch-3D correspondences. The first key contribution of this paper is two new datasets, consisting a total of 4,680 sketch-3D pairings from two object categories. Even with the datasets, FG-SBSR is still highly challenging because (i) the inherent domain gap between 2D sketch and 3D shape is large, and (ii) retrieval needs to be conducted at the instance level instead of the coarse category level matching as in traditional SBSR. Thus, the second contribution of the paper is the first cross-modal deep embedding model for FG-SBSR, which specifically tackles the unique challenges presented by this new problem. Core to the deep embedding model is a novel cross-modal view attention module which automatically computes the optimal combination of 2D projections of a 3D shape given a query sketch."
  publ_id: "2021_1"
  type: "j"
  

  

- title: "Towards Practical Sketch-based 3D Shape Generation: The Role of Professional Sketches"
  title_link: ""
  authors: "Yue Zhong, Yonggang Qi, <b>Yulia Gryaditskaya</b>, Honggang Zhang, Yi-Zhe Song"
  venue: "IEEE Transactions on Circuits and Systems for Video Technology"
  year: 2020  
  paper_link: "https://ieeexplore.ieee.org/iel7/76/9527373/09272370.pdf?casa_token=plKho3sVLQoAAAAA:n7gt0lSAxCR-AmkG9xoNdm_fKH5pCqfRA7HEv7uzk-I6_VCRX5NqYFO5rtlWaZmgHw0VUJ11rA"
  project_link: "https://github.com/Yueeey/sketcch3D"
  preview_img_link: "./Papers/ProSketch/preview.png"
  img_width: '130px'
  bibtex_link: "/Papers/ProSketch/zhong2020towards.txt"
  abstract: "In this paper, for the first time, we investigate the problem of generating 3D shapes from professional 2D sketches via deep learning. We target sketches done by professional artists, as these sketches are likely to contain more details than the ones produced by novices, and thus the reconstruction from such sketches poses a higher demand on the level of detail in the reconstructed models. This is importantly different to previous work, where the training and testing was conducted on either synthetic sketches or sketches done by novices. Novices sketches often depict shapes that are physically unrealistic, while models trained with synthetic sketches could not cope with the level of abstraction and style found in real sketches. To address this problem, we collected the first large-scale dataset of professional sketches, where each sketch is paired with a reference 3D shape, with a total of 1,500 professional sketches collected across 500 3D shapes. The dataset is available at http://sketchx.ai/downloads/. We introduce two bespoke designs within a deep adversarial network to tackle the imprecision of human sketches and the unique figure/ground ambiguity problem inherent to sketch-based reconstruction. We show that existing 3D shapes generation methods designed for images fail to be naively applied to our problem, and demonstrate the effectiveness of our method both qualitatively and quantitatively."
  publ_id: "2020_1"
  type: "j"
  
- title: "Towards 3D VR-Sketch to 3D Shape Retrieval"
  title_link: "https://tinyurl.com/3DSketch3DV"
  authors: "Ling Luo, <b>Yulia Gryaditskaya</b>, Yongxin Yang, Tao Xiang, Yi-Zhe Song"
  venue: "Proceedings of International Conference on 3D Vision (3DV) - (Oral)"
  year: 2020  
  paper_link: "https://ieeexplore.ieee.org/document/9320353"
  preview_img_link: "./Papers/3DV/Sketch3D/preview.png"
  img_width: '110px'  
  project_link: "https://github.com/ygryadit/Towards3DVRSketch"
  bibtex_link: "/Papers/3DV/Sketch3D/luo2020towards.txt"  
  publ_id: "2020_2"
  abstract: "Growing free online 3D shapes collections dictated research on 3D retrieval. Active debate has however been had
on (i) what the best input modality is to trigger retrieval,
and (ii) the ultimate usage scenario for such retrieval. In
this paper, we offer a different perspective towards answering these questions – we study the use of 3D sketches as
an input modality and advocate a VR-scenario where retrieval is conducted. Thus, the ultimate vision is that users
can freely retrieve a 3D model by air-doodling in a VR environment. As a first stab at this new 3D VR-sketch to 3D
shape retrieval problem, we make four contributions. First,
we code a VR utility to collect 3D VR-sketches and conduct
retrieval. Second, we collect the first set of 167 3D VRsketches on two shape categories from ModelNet. Third, we
propose a novel approach to generate a synthetic dataset of
human-like 3D sketches of different abstract levels to train
deep networks. At last, we compare the common multi-view
and volumetric approaches: We show that, in contrast to
3D shape to 3D shape retrieval, volumetric point-based approaches exhibit superior performance on 3D sketch to 3D
shape retrieval due to the sparse and abstract nature of
3D VR-sketches. We believe these contributions will collectively serve as enablers for future attempts at this problem."
  type: "c"
  
- title: "Deep Sketch-Based Modeling: Tips and Tricks"
  preview_img_link: "./Papers/3DV/DeepSketch/preview.png"
  img_width: '110px'  
  title_link: "https://tinyurl.com/DeepSketchModeling"
  authors: "Yue Zhong, <b>Yulia Gryaditskaya</b>, Honggang Zhang, Yi-Zhe Song"
  venue: "Proceedings of International Conference on 3D Vision (3DV) - (Spotlight)"
  year: 2020  
  paper_link: "https://arxiv.org/pdf/2011.06133.pdf"
  project_link: "https://tinyurl.com/DeepSketchModeling"
  bibtex_link: "/Papers/3DV/DeepSketch/zhong2020deep.txt"
  publ_id: "2020_3"
  abstract: "Deep image-based modeling received lots of attention in recent years, yet the parallel problem of sketch-based modeling has only been briefly studied, often as a potential application. In this work, for the first time, we identify the main differences between sketch and image inputs: (i) style variance, (ii) imprecise perspective, and (iii) sparsity. We discuss why each of these differences can pose a challenge, and even make a certain class of image-based methods inapplicable. We study alternative solutions to address each of the difference. By doing so, we drive out a few important insights: (i) sparsity commonly results in an incorrect prediction of foreground versus background, (ii) diversity of human styles, if not taken into account, can lead to very poor generalization properties, and finally (iii) unless a dedicated sketching interface is used, one can not expect sketches to match a perspective of a fixed viewpoint. Finally, we compare a set of representative deep single-image modeling solutions and show how their performance can be improved to tackle sketch input by taking into consideration the identified critical differences."
  type: "c"
  
- title: "Lifting Freehand Concept Sketches into 3D"
  preview_img_link: "./Papers/Lift3D/preview.png"
  img_width: '110px'  
  title_link: "https://ns.inria.fr/d3/Lift3D"
  authors: "<b>Yulia Gryaditskaya</b>, Felix Hähnlein, <a href='https://www.cs.ubc.ca/~chenxil/'>Chenxi Liu</a>, <a href='https://www.cs.ubc.ca/~sheffa/'>Alla Sheffer</a>, <a href='http://www-sop.inria.fr/members/Adrien.Bousseau/'>Adrien Bousseau</a>"
  venue: "ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia)"
  year: 2020  
  paper_link: "https://repo-sam.inria.fr/d3/Lift3D/Gryaditskaya_SigAsia20_Lifting%20_Freehand_Concept_Sketches_into_3D.pdf"
  project_link: "https://ns.inria.fr/d3/Lift3D"
  bibtex_link: "/Papers/Lift3D/gryaditskaya2020lifting.txt"
  publ_id: "2020_5"
  abstract: "We present the first algorithm capable of automatically lifting real-world, vector-format, industrial design sketches into 3D. Targeting real-world sketches raises numerous challenges due to inaccuracies, use of overdrawn strokes, and construction lines. In particular, while construction lines convey important 3D information, they add significant clutter and introduce multiple accidental 2D intersections. Our algorithm exploits the geometric cues
provided by the construction lines and lifts them to 3D by computing their
intended 3D intersections and depths. Once lifted to 3D, these lines provide
valuable geometric constraints that we leverage to infer the 3D shape of
other artist drawn strokes. The core challenge we address is inferring the
3D connectivity of construction and other lines from their 2D projections by
separating 2D intersections into 3D intersections and accidental occlusions.
We efficiently address this complex combinatorial problem using a dedicated
search algorithm that leverages observations about designer drawing preferences, and uses those to explore only the most likely solutions of the 3D
intersection detection problem. We demonstrate that our separator outputs
are of comparable quality to human annotations, and that the 3D structures
we recover enable a range of design editing and visualization applications,
including novel view synthesis and 3D-aware scaling of the depicted shape."
  type: "j"
  
- title: "Pixelor: A Competitive Sketching AI Agent. So you think you can sketch?"
  preview_img_link: "./Papers/Pixelor/preview.jpg"
  img_width: '110px'  
  title_link: "http://sketchx.ai/pixelor"
  authors: "Ayan Kumar Bhunia*, Ayan Das*, Umar Riaz Muhammad*, Yongxin Yang, Timothy Hospedales, Tao Xiang, <b>Yulia Gryaditskaya</b>, Yi-Zhe Song <br />
               *Joint first authour"
  venue: "ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia)"
  year: 2020  
  paper_link: "https://dl.acm.org/doi/pdf/10.1145/3414685.3417840"
  project_link: "http://sketchx.ai/pixelor"
  bibtex_link: "/Papers/Pixelor/bhunia2020pixelor.txt"
  abstract: "We present the first competitive drawing agent Pixelor that exhibits humanlevel performance at a Pictionary-like sketching game, where the participant
whose sketch is recognized first is a winner. Our AI agent can autonomously
sketch a given visual concept, and achieve a recognizable rendition as quickly
or faster than a human competitor. The key to victory for the agent’s goal
is to learn the optimal stroke sequencing strategies that generate the most
recognizable and distinguishable strokes first. Training Pixelor is done in two
steps. First, we infer the stroke order that maximizes early recognizability of
human training sketches. Second, this order is used to supervise the training
of a sequence-to-sequence stroke generator. Our key technical contributions
are a tractable search of the exponential space of orderings using neural
sorting; and an improved Seq2Seq Wasserstein (S2S-WAE) generator that
uses an optimal-transport loss to accommodate the multi-modal nature of the
optimal stroke distribution. Our analysis shows that Pixelor is better than the
human players of the Quick, Draw! game, under both AI and human judging
of early recognition. To analyze the impact of human competitors’ strategies,
we conducted a further human study with participants being given unlimited
thinking time and training in early recognizability by feedback from an AI
judge. The study shows that humans do gradually improve their strategies with training, but overall Pixelor still matches human performance."
  publ_id: "2020_4"
  type: "j"
  
  
- title: "OpenSketch: A Richly-Annotated Dataset of Product Design Sketches"
  preview_img_link: "./Papers/OpenSketch/preview_open_sketch.png"
  img_width: '110px'  
  title_link: "https://ns.inria.fr/d3/OpenSketch/"
  authors: "<b>Yulia Gryaditskaya</b>, <a href='https://www.marksypesteyn.com/'>Mark Sypesteyn</a>, <a href='https://www.linkedin.com/in/janwillemhoftijzer'>Jan Willem Hoftijzer</a>, <a href='https://www.tudelft.nl/en/ide/about-ide/personal-profiles/professors/pont-sc/'>Sylvia Pont</a>, <a href='http://people.csail.mit.edu/fredo/'>Fr&eacute;do Durand</a>, <a href='http://www-sop.inria.fr/members/Adrien.Bousseau/'>Adrien Bousseau</a>"
  venue: "ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia)"
  year: 2019  
  paper_link: "https://repo-sam.inria.fr/d3/OpenSketch/Gryaditskaya_OpenSketch_AuthorsVersion.pdf"
  project_link: "https://repo-sam.inria.fr/d3/OpenSketch/"
  bibtex_link: "/Papers/OpenSketch/gryaditskaya2019opensketch.txt"
  abstract: "Product designers extensively use sketches to create and communicate 3D shapes and thus form an ideal audience for sketch-based modeling, non-photorealistic rendering and sketch filtering. However, sketching requires significant expertise and time, making design sketches a scarce resource for the research community. We introduce OpenSketch, a dataset of product design sketches aimed at offering a rich source of information for a variety of computer-aided design tasks. OpenSketch contains more than 400 sketches representing 12 man-made objects drawn by 7 to 15 product designers of varying expertise. We provided participants with front, side and top views of these objects, and instructed them to draw from two novel perspective viewpoints. This drawing task forces designers to construct the shape from their mental vision rather than directly copy what they see. They achieve this task by employing a variety of sketching techniques and methods not observed in prior datasets. Together with industrial design teachers, we distilled a taxonomy of line types and used it to label each stroke of the 214 sketches drawn from one of the two viewpoints. While some of these lines have long been known in computer graphics, others remain to be reproduced algorithmically or exploited for shape inference. In addition, we also asked participants to produce clean presentation drawings from each of their sketches, resulting in aligned pairs of drawings of different styles. Finally, we registered each sketch to its reference 3D model by annotating sparse correspondences. We provide an analysis of our annotated sketches, which reveals systematic drawing strategies over time and shapes, as well as a positive correlation between presence of construction lines and accuracy. Our sketches, in combination with provided annotations, form challenging benchmarks for existing algorithms as well as a great source of inspiration for future developments. We illustrate the versatility of our data by using it to test a 3D reconstruction deep network trained on synthetic drawings, as well as to train a filtering network to convert concept sketches into presentation drawings."
  publ_id: "2019_2"
  type: "j"
  
- title: "Bitmap or Vector? A study on sketch representations for deep stroke segmentation"
  preview_img_link: "./Papers/BitmapVector/preview.png"
  img_width: '110px'  
  title_link: "https://hal.inria.fr/hal-02922043/"
  authors: "Felix Hähnlein, <a href='http://yulia.gryaditskaya.com/'><b>Yulia Gryaditskaya</b></a>, <a href='http://www-sop.inria.fr/members/Adrien.Bousseau/'>Adrien Bousseau</a>"
  venue: "Journées Francaises d'Informatique Graphique et de Réalité virtuelle"
  year: 2019  
  paper_link: "https://hal.inria.fr/hal-02922043/document"
  project_link: ""
  # bibtex_link: "./Papers/BitmapVectorhahnlein2019bitmap.txt"
  publ_id: "2019_1"
  type: "c"

- title: "High Dynamic Range Imaging: Problems of Video Exposure Bracketing, Luminance Calibration and Gloss Editing"
  preview_img_link: "./Thesis/preview.png"
  img_width: '110px'  
  title_link: "https://diglib.eg.org/handle/10.2312/2631883"
  authors: "<b>Yulia Gryaditskaya</b>"
  venue: "PhD Thesis, Faculty of Mathematics and Computer Science of Saarland University"
  year: 2017  
  paper_link: "https://diglib.eg.org/bitstream/handle/10.2312/2631883/thesis_gryaditskaya_compressed.pdf?sequence=1&isAllowed=y"
  project_link: ""
  # bibtex_link: "./Thesis/gryaditskaya2017thesis.txt"
  publ_id: "2017_1"
  type: "d"
  
- title: "Gloss Editing in Light Fields"
  preview_img_link: "Papers/GlossEditing/preview.png"
  img_width: '110px'  
  title_link: "Papers/GlossEditing/index.html#vmv1"
  authors: "<a href='http://yulia.gryaditskaya.com/'><b>Yulia Gryaditskaya</b></a>, <a href='http://webdiis.unizar.es/~bmasia/'>Belen Masia</a>, <a href='https://www.pdf.inf.usi.ch/people/piotr/'>Piotr Didyk</a>, <a href='https://people.mpi-inf.mpg.de/~karol/'>Karol Myszkowski</a>,<a href='https://people.mpi-inf.mpg.de/~hpseidel/'>Hans-Peter Seidel</a>"
  venue: " Proceedings of International Workshop on Vision, Modeling and Visualization (VMV)"
  year: 2016  
  paper_link: "Papers/GlossEditing/2016_Gryaditskaya_GlossEditingLF.pdf"
  project_link: "Papers/GlossEditing/index.html#vmv1"
  bibtex_link: "Papers/GlossEditing/gryaditskaya2016gloss.txt"
  publ_id: "2016_1"
  type: "c"

- title: "Motion Aware Exposure Bracketing for HDR Video"
  preview_img_link: "Papers/HDRVideo/preview.png"
  img_width: '110px'  
  title_link: "Papers/HDRVideo/index.html#egsr15"
  authors: " <a href='http://yulia.gryaditskaya.com/'><b>Yulia Gryaditskaya</b></a>, <a href='http://taniapouli.me/'>Foteini Tania Pouli</a>, <a href='hhttp://erikreinhard.com/about.html'>Erik Reinhard</a>, <a href='https://people.mpi-inf.mpg.de/~karol/'>Karol Myszkowski</a>, <a href='https://people.mpi-inf.mpg.de/~hpseidel/'>Hans-Peter Seidel</a>"
  venue: " Computer Graphics Forum (Proc. EGSR)"
  year: 2015  
  paper_link: "Papers/HDRVideo/ExposureBracketingHDRVideo.pdf"
  project_link: "Papers/HDRVideo/index.html#egsr15"
  bibtex_link: "Papers/HDRVideo/gryaditskaya2015motion.txt"
  publ_id: "2015_1"
  type: "j"

- title: "Sky Based Light Metering for High Dynamic Range Images"
  preview_img_link: "Papers/LightMetering/preview.png"
  img_width: '110px'  
  title_link: "Papers/LightMetering/index.html#pg14"
  authors: "<a href='http://yulia.gryaditskaya.com/'><b>Yulia Gryaditskaya</b></a>, <a href='http://taniapouli.me/'>Foteini Tania Pouli</a>, <a href='hhttp://erikreinhard.com/about.html'>Erik Reinhard</a>, <a href='https://people.mpi-inf.mpg.de/~hpseidel/'>Hans-Peter Seidel</a>"
  venue: " Computer Graphics Forum (Proc. Pacific Graphics)"
  year: 2014  
  paper_link: "Papers/LightMetering/LM_Supp.pdf"
  project_link: "Papers/LightMetering/index.html#pg14"
  bibtex_link: "Papers/LightMetering/gryaditskaya2014sky.txt"
  publ_id: "2014_1"  
  type: "j"